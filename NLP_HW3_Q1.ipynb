{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AUT-Student/NLP-HW3/blob/main/NLP_HW3_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><b>In the name of God</b></center>\n",
        "\n",
        "<b>Course</b>: Natural Language Processing\n",
        "<br>\n",
        "<b>Description:</b> HomeWork 3 | Question 1\n",
        "<br>\n",
        "<b>Developer</b>: Alireza Mazochi (400131075)"
      ],
      "metadata": {
        "id": "a4zafnKkLHr-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoKwrs6nALlb"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uScG-ckwCRBc",
        "outputId": "58eee32b-fed8-498c-e734-ce238f883b27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert_embedding\n",
            "  Downloading bert_embedding-1.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting mxnet==1.4.0\n",
            "  Downloading mxnet-1.4.0-py2.py3-none-manylinux1_x86_64.whl (29.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.6 MB 96.5 MB/s \n",
            "\u001b[?25hCollecting gluonnlp==0.6.0\n",
            "  Downloading gluonnlp-0.6.0.tar.gz (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 59.7 MB/s \n",
            "\u001b[?25hCollecting numpy==1.14.6\n",
            "  Downloading numpy-1.14.6-cp37-cp37m-manylinux1_x86_64.whl (13.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8 MB 39.7 MB/s \n",
            "\u001b[?25hCollecting typing==3.6.6\n",
            "  Downloading typing-3.6.6-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet==1.4.0->bert_embedding) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (2022.5.18.1)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.6.0-py3-none-any.whl size=259930 sha256=6bf5192959dc5d7c3e55a703c2f8c2798a7d3312b2034ed977fe86005c9fa33e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/41/8f/45bd1c58055d87aee5a71b6756a427ea8d92e506b3a9d17370\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: numpy, graphviz, typing, mxnet, gluonnlp, bert-embedding\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires numpy>=1.16.0, but you have numpy 1.14.6 which is incompatible.\n",
            "xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.14.6 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.14.6 which is incompatible.\n",
            "tifffile 2021.11.2 requires numpy>=1.15.1, but you have numpy 1.14.6 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires numpy>=1.20, but you have numpy 1.14.6 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.14.6 which is incompatible.\n",
            "spacy 2.2.4 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "seaborn 0.11.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.14.6 which is incompatible.\n",
            "pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.14.6 which is incompatible.\n",
            "pymc3 3.11.4 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.14.6 which is incompatible.\n",
            "plotnine 0.6.0 requires numpy>=1.16.0, but you have numpy 1.14.6 which is incompatible.\n",
            "pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.14.6 which is incompatible.\n",
            "numba 0.51.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "librosa 0.8.1 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.14.6 which is incompatible.\n",
            "jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.14.6 which is incompatible.\n",
            "jax 0.3.8 requires numpy>=1.19, but you have numpy 1.14.6 which is incompatible.\n",
            "imgaug 0.2.9 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "fbprophet 0.7.1 requires numpy>=1.15.4, but you have numpy 1.14.6 which is incompatible.\n",
            "fastai 1.0.61 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "cvxpy 1.0.31 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "blis 0.4.1 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed bert-embedding-1.0.1 gluonnlp-0.6.0 graphviz-0.8.4 mxnet-1.4.0 numpy-1.14.6 typing-3.6.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install bert_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "hA9r0mMC_3Jr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from types import SimpleNamespace\n",
        "import prettytable\n",
        "from prettytable import PrettyTable\n",
        "from bert_embedding import BertEmbedding\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHwqZrZoANyC"
      },
      "source": [
        "# Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fpa8eFG7AKkQ",
        "outputId": "0b38ed97-5498-4cc7-ddac-85d9ae2e08c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oin_Sw1Gk_WLS9zpDrap5FdlRcUrTP_D\n",
            "To: /content/Sentenses_train.txt\n",
            "100% 1.23M/1.23M [00:00<00:00, 125MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EvtGQ8-sYXXQ3VA9ByjD4OmFH13WxvAs\n",
            "To: /content/Senses_train.txt\n",
            "100% 49.7k/49.7k [00:00<00:00, 46.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=154f-z0PsPAp0yvOLdNXP8NXYjPljpgCZ\n",
            "To: /content/Sentenses_test.txt\n",
            "100% 213k/213k [00:00<00:00, 86.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EDG_j6F5ohIjpkihRQd0-i9daHWV7StN\n",
            "To: /content/Senses_test.txt\n",
            "100% 8.76k/8.76k [00:00<00:00, 10.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1oin_Sw1Gk_WLS9zpDrap5FdlRcUrTP_D\n",
        "!gdown 1EvtGQ8-sYXXQ3VA9ByjD4OmFH13WxvAs\n",
        "!gdown 154f-z0PsPAp0yvOLdNXP8NXYjPljpgCZ\n",
        "!gdown 1EDG_j6F5ohIjpkihRQd0-i9daHWV7StN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lvRliKivDtSj"
      },
      "outputs": [],
      "source": [
        "def create_dataset(part_name):\n",
        "  sentences_file = open(f\"/content/Sentenses_{part_name}.txt\", \"r\")\n",
        "  senses_file = open(f\"/content/Senses_{part_name}.txt\", \"r\")\n",
        "\n",
        "  sentences = sentences_file.readlines()\n",
        "  senses = senses_file.readlines()\n",
        "  \n",
        "  dataset_sentence = []\n",
        "  dataset_sense = []\n",
        "  dataset_ambiguity_word = []\n",
        "\n",
        "  for sentence, sense in list(zip(sentences, senses)):\n",
        "    dataset_sense.append(sense[:-1])\n",
        "    tokens = sentence.split()\n",
        "    word_index = tokens.index(\"<head>\")\n",
        "    dataset_ambiguity_word.append(tokens[word_index+1].lower())\n",
        "    dataset_sentence.append(\" \".join(\n",
        "        tokens[max(0, word_index-11):word_index-1] +\\\n",
        "        [tokens[word_index+1]] +\\\n",
        "        tokens[word_index+3:min(word_index+13, len(tokens))])\n",
        "    )\n",
        "  \n",
        "  return pd.DataFrame({\"Sentence\": dataset_sentence, \"Ambiguity_Word\": dataset_ambiguity_word, \"Sense\": dataset_sense})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wGM-yyXsHMS2"
      },
      "outputs": [],
      "source": [
        "train_dataset = create_dataset(\"train\")\n",
        "test_dataset = create_dataset(\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBPgkfq2DZ0f"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Yc9cmJRjMH",
        "outputId": "e2117295-cb9e-4ee1-cbe9-f059f776ccf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab file is not found. Downloading.\n",
            "Downloading /root/.mxnet/models/book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n",
            "Downloading /root/.mxnet/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip...\n"
          ]
        }
      ],
      "source": [
        "# GPU\n",
        "# ctx = mx.gpu(0)\n",
        "# bert_embedding = BertEmbedding(ctx=ctx)\n",
        "\n",
        "# CPU\n",
        "bert_embedding = BertEmbedding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AgP2y3gHTeJA"
      },
      "outputs": [],
      "source": [
        "def calculate_ambiguity_vector(dataset):\n",
        "  dataset[\"Ambiguity_Vector\"] = None\n",
        "  for i, data in dataset.iterrows():\n",
        "    if i%300==0: print(f\"{int(i*100/len(dataset))}%\")\n",
        "    tokens, vectors = bert_embedding([data[\"Sentence\"]])[0]\n",
        "    ambiguity_word = data[\"Ambiguity_Word\"]\n",
        "\n",
        "    try:    \n",
        "      ambiguity_vector = vectors[tokens.index(ambiguity_word)]\n",
        "    except ValueError:\n",
        "      tokens, vectors = bert_embedding([data[\"Sentence\"][15:]])[0]\n",
        "      ambiguity_word = data[\"Ambiguity_Word\"]\n",
        "      ambiguity_vector = vectors[tokens.index(ambiguity_word)]\n",
        "\n",
        "    dataset.at[i, \"Ambiguity_Vector\"] = ambiguity_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noqqFDpATGXL",
        "outputId": "747fe314-c29f-4083-8b2e-e1da63e9c762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0%\n",
            "4%\n",
            "9%\n",
            "14%\n",
            "18%\n",
            "23%\n",
            "28%\n",
            "33%\n",
            "37%\n",
            "42%\n",
            "47%\n",
            "52%\n",
            "56%\n",
            "61%\n",
            "66%\n",
            "71%\n",
            "75%\n",
            "80%\n",
            "85%\n",
            "90%\n",
            "94%\n",
            "99%\n"
          ]
        }
      ],
      "source": [
        "calculate_ambiguity_vector(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoT3FYpqJIi4",
        "outputId": "456cdab9-0cec-4fb7-e7a0-d1fa471049b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0%\n",
            "26%\n",
            "53%\n",
            "80%\n"
          ]
        }
      ],
      "source": [
        "calculate_ambiguity_vector(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3I3pghNpV00"
      },
      "source": [
        "# Dimension Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "c1fRsHBXpYRY"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfjcsVjkqVX6",
        "outputId": "e1a886f0-6759-4a9c-8c12-70fe665df75c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(n_components=300)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "pca.fit(np.array([item for item in train_dataset[\"Ambiguity_Vector\"].values]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igV66t4018fO",
        "outputId": "acdf01f4-fcbb-45d7-a70d-bfcf5c807c21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6324, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "pca.transform(np.array([item for item in train_dataset[\"Ambiguity_Vector\"].values])).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jOgKRFlj1aia"
      },
      "outputs": [],
      "source": [
        "train_dataset[\"Ambiguity_Vector_Reduced\"] = [item for item in pca.transform(np.array([item for item in train_dataset[\"Ambiguity_Vector\"].values]))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Yuu9B5ow2LW5"
      },
      "outputs": [],
      "source": [
        "test_dataset[\"Ambiguity_Vector_Reduced\"] = [item for item in pca.transform(np.array([item for item in test_dataset[\"Ambiguity_Vector\"].values]))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE9LeGvuYDgC"
      },
      "source": [
        "# Train Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yynx9hOihJ7O",
        "outputId": "e1cacbd2-d1db-4ef2-b7bc-83033d1354f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'HARD1',\n",
              " 'HARD2',\n",
              " 'HARD3',\n",
              " 'SERVE10',\n",
              " 'SERVE12',\n",
              " 'cord',\n",
              " 'division',\n",
              " 'formation',\n",
              " 'interest1',\n",
              " 'interest2',\n",
              " 'interest3',\n",
              " 'interest4',\n",
              " 'interest5',\n",
              " 'interest6',\n",
              " 'phone',\n",
              " 'product',\n",
              " 'text'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "set(train_dataset[\"Sense\"].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZQJtynDiimx",
        "outputId": "77b870fa-f400-45d2-c509-af29fe752961"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hard',\n",
              " 'harder',\n",
              " 'hardest',\n",
              " 'interest',\n",
              " 'interested',\n",
              " 'interesting',\n",
              " 'interests',\n",
              " 'line',\n",
              " 'lines',\n",
              " 'serve',\n",
              " 'served',\n",
              " 'serves'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "set(train_dataset[\"Ambiguity_Word\"].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MbFhDMIZhr9j"
      },
      "outputs": [],
      "source": [
        "hard_words = [\"hard\", \"harder\", \"hardest\"]\n",
        "interest_words = [\"interest\", \"interested\", \"interesting\", \"interests\"]\n",
        "line_words = [\"line\", \"lines\"]\n",
        "serve_words = [\"serve\", \"served\", \"serves\"]\n",
        "\n",
        "hard_senses = [\"HARD1\", \"HARD2\", \"HARD3\"]\n",
        "interest_senses = [\"interest1\", \"interest2\", \"interest3\", \"interest4\", \"interest5\", \"interest6\"]\n",
        "line_senses = [\"cord\", \"division\", \"formation\", \"phone\", \"product\", \"text\"]\n",
        "serve_senses = [\"SERVE10\", \"SERVE12\"]\n",
        "\n",
        "mapping_word_sense = {\n",
        "    \"HARD\": (hard_words, hard_senses),\n",
        "    \"INTEREST\": (interest_words, interest_senses),\n",
        "    \"LINE\": (line_words, line_senses),\n",
        "    \"SERVE\": (serve_words, serve_senses)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset = train_dataset.sample(frac=0.2, random_state=0)\n",
        "train_dataset = train_dataset.drop(valid_dataset.index)"
      ],
      "metadata": {
        "id": "3mBJ7zUc0RGb"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "YiKyO1IDBOrU"
      },
      "outputs": [],
      "source": [
        "def report_accuracy_f(classifier, dataset, verbose=False):\n",
        "  y_predict = classifier.predict(np.stack(dataset[\"Ambiguity_Vector_Reduced\"].values))\n",
        "  y_true = dataset[\"Sense\"].values\n",
        "\n",
        "  accuracy = round(accuracy_score(y_true, y_predict)*100, 2)\n",
        "  f1 = round(f1_score(y_true, y_predict, average='macro')*100, 2)\n",
        "\n",
        "  if verbose:\n",
        "    print(f\"Accuracy = {accuracy}\")\n",
        "    print(f\"F1 = {f1}\")  \n",
        "  \n",
        "  return accuracy, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "n7Fd83Yu4huD"
      },
      "outputs": [],
      "source": [
        "classifiers = {}\n",
        "\n",
        "settings = [\n",
        "            {\"kernel\": \"linear\"},\n",
        "            {\"kernel\": \"poly\", \"degree\":3},\n",
        "            {\"kernel\": \"poly\", \"degree\":6},\n",
        "            {\"kernel\": \"rbf\"}\n",
        "]\n",
        "\n",
        "for key in mapping_word_sense.keys():\n",
        "  words = mapping_word_sense[key][0]\n",
        "  senses = mapping_word_sense[key][1]\n",
        "\n",
        "  sub_train_dataset = train_dataset[train_dataset[\"Ambiguity_Word\"].isin(words) & train_dataset[\"Sense\"].isin(senses)]\n",
        "  \n",
        "  sub_valid_dataset = valid_dataset[valid_dataset[\"Ambiguity_Word\"].isin(words) & valid_dataset[\"Sense\"].isin(senses)]\n",
        "\n",
        "  if key == \"HARD\":      \n",
        "    classifier = SimpleNamespace(predict=lambda x:np.array([\"HARD1\"]*len(x)))\n",
        "\n",
        "    classifiers[key] = {\"classifier\": classifier, \"setting\": None}  \n",
        "  else:\n",
        "\n",
        "    best_accuracy = 0\n",
        "    best_classifier = None\n",
        "    best_setting = None\n",
        "\n",
        "    for setting in settings:\n",
        "      classifier = SVC(**setting)\n",
        "      classifier.fit(X=np.stack(sub_train_dataset[\"Ambiguity_Vector_Reduced\"].values), y=sub_train_dataset[\"Sense\"].values)\n",
        "      \n",
        "      accuracy = report_accuracy_f(classifier, sub_valid_dataset)[0]\n",
        "\n",
        "      if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_classifier = classifier\n",
        "        best_setting = setting\n",
        "\n",
        "    classifiers[key] = {\"classifier\": best_classifier, \"setting\": best_setting}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table = PrettyTable()\n",
        "table.field_names = [\"Word\", \"Setting\", \"Train Number\", \"Valid Number\", \"Test Number\",\n",
        "                     \"Train Accuracy\", \"Valid Accuracy\", \"Test Accuracy\",\n",
        "                     \"Train F1\", \"Valid F1\", \"Test F1\"]\n",
        "\n",
        "sum_train_number, sum_valid_number, sum_test_number = 0, 0, 0\n",
        "sum_train_accuracy, sum_valid_accuracy, sum_test_accuracy= 0, 0, 0\n",
        "sum_train_f1, sum_valid_f1, sum_test_f1 = 0, 0, 0\n",
        "\n",
        "for key in mapping_word_sense.keys():\n",
        "  words = mapping_word_sense[key][0]\n",
        "  senses = mapping_word_sense[key][1]\n",
        "\n",
        "  sub_train_dataset = train_dataset[train_dataset[\"Ambiguity_Word\"].isin(words) & train_dataset[\"Sense\"].isin(senses)]\n",
        "  sub_valid_dataset = valid_dataset[valid_dataset[\"Ambiguity_Word\"].isin(words) & valid_dataset[\"Sense\"].isin(senses)]\n",
        "  sub_test_dataset = test_dataset[test_dataset[\"Ambiguity_Word\"].isin(words) & test_dataset[\"Sense\"].isin(senses)]\n",
        "  \n",
        "  classifier = classifiers[key][\"classifier\"]\n",
        "  setting = classifiers[key][\"setting\"]\n",
        "\n",
        "  train_number = len(sub_train_dataset)\n",
        "  valid_number = len(sub_valid_dataset)\n",
        "  test_number = len(sub_test_dataset)\n",
        "\n",
        "  sum_train_number += train_number\n",
        "  sum_valid_number += valid_number\n",
        "  sum_test_number += test_number\n",
        "\n",
        "  train_accuracy, train_f1 = report_accuracy_f(classifier, sub_train_dataset)\n",
        "  valid_accuracy, valid_f1 = report_accuracy_f(classifier, sub_valid_dataset)\n",
        "  test_accuracy, test_f1 = report_accuracy_f(classifier, sub_test_dataset)\n",
        "\n",
        "  sum_train_accuracy += train_accuracy * train_number\n",
        "  sum_valid_accuracy += valid_accuracy * valid_number\n",
        "  sum_test_accuracy += test_accuracy * test_number\n",
        "  \n",
        "  sum_train_f1 += train_f1 * train_number\n",
        "  sum_valid_f1 += valid_f1 * valid_number\n",
        "  sum_test_f1 += test_f1 * test_number\n",
        "  \n",
        "  table.add_row([key, setting, train_number, valid_number, test_number,\n",
        "                 train_accuracy, valid_accuracy, test_accuracy,\n",
        "                 train_f1, valid_f1, test_f1])\n",
        "\n",
        "avg_train_accuracy = round(sum_train_accuracy/sum_train_number, 2)\n",
        "avg_valid_accuracy = round(sum_valid_accuracy/sum_valid_number, 2)\n",
        "avg_test_accuracy = round(sum_test_accuracy/sum_test_number, 2)\n",
        "\n",
        "avg_train_f1 = round(sum_train_f1/sum_train_number, 2)\n",
        "avg_valid_f1 = round(sum_valid_f1/sum_valid_number, 2)\n",
        "avg_test_f1 = round(sum_test_f1/sum_test_number, 2)\n",
        "\n",
        "table.add_row([\"Overall\", None, sum_train_number, sum_valid_number, sum_test_number,\n",
        "               avg_train_accuracy, avg_valid_accuracy, avg_test_accuracy,\n",
        "               avg_train_f1, avg_valid_f1, avg_test_f1\n",
        "              ])\n",
        "\n",
        "print(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUPJk15vxWS3",
        "outputId": "78267088-cfd2-4404-d82d-7d2db3d2528d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------------+--------------+--------------+-------------+----------------+----------------+---------------+----------+----------+---------+\n",
            "|   Word   |       Setting        | Train Number | Valid Number | Test Number | Train Accuracy | Valid Accuracy | Test Accuracy | Train F1 | Valid F1 | Test F1 |\n",
            "+----------+----------------------+--------------+--------------+-------------+----------------+----------------+---------------+----------+----------+---------+\n",
            "|   HARD   |         None         |     1334     |     327      |     300     |     100.0      |     100.0      |     100.0     |  100.0   |  100.0   |  100.0  |\n",
            "| INTEREST | {'kernel': 'linear'} |     1343     |     337      |     285     |     100.0      |     91.69      |     89.82     |  100.0   |  62.96   |  74.61  |\n",
            "|   LINE   |  {'kernel': 'rbf'}   |     1185     |     270      |     236     |     97.38      |     92.59      |     94.49     |   97.2   |  92.74   |  94.09  |\n",
            "|  SERVE   | {'kernel': 'linear'} |     1112     |     304      |     276     |     100.0      |     100.0      |     100.0     |  100.0   |  100.0   |  100.0  |\n",
            "| Overall  |         None         |     4974     |     1238     |     1097    |     99.38      |     96.12      |     96.17     |  99.33   |  88.33   |  92.13  |\n",
            "+----------+----------------------+--------------+--------------+-------------+----------------+----------------+---------------+----------+----------+---------+\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP-HW3-Q1.ipynb",
      "provenance": [],
      "mount_file_id": "18Dds__YuJRtt7GQQm2DLLALdHL0K1l3l",
      "authorship_tag": "ABX9TyPVk0e+lLPnXeBPj8fhQhrf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}