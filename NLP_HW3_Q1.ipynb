{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-HW3-Q1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPFuWy2e/GFztZSatXo9rM3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AUT-Student/NLP-HW3/blob/main/NLP_HW3_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "RoKwrs6nALlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uScG-ckwCRBc",
        "outputId": "a6ccb9e1-6132-4764-e98e-79a9065c77f9"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bert_embedding in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Collecting numpy==1.14.6\n",
            "  Using cached numpy-1.14.6-cp37-cp37m-manylinux1_x86_64.whl (13.8 MB)\n",
            "Requirement already satisfied: typing==3.6.6 in /usr/local/lib/python3.7/dist-packages (from bert_embedding) (3.6.6)\n",
            "Requirement already satisfied: gluonnlp==0.6.0 in /usr/local/lib/python3.7/dist-packages (from bert_embedding) (0.6.0)\n",
            "Requirement already satisfied: mxnet==1.4.0 in /usr/local/lib/python3.7/dist-packages (from bert_embedding) (1.4.0)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet==1.4.0->bert_embedding) (2.23.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet==1.4.0->bert_embedding) (0.8.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (1.24.3)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "yellowbrick 1.4 requires numpy>=1.16.0, but you have numpy 1.14.6 which is incompatible.\n",
            "xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.14.6 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.14.6 which is incompatible.\n",
            "tifffile 2021.11.2 requires numpy>=1.15.1, but you have numpy 1.14.6 which is incompatible.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires numpy>=1.20, but you have numpy 1.14.6 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.14.6 which is incompatible.\n",
            "spacy 2.2.4 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "seaborn 0.11.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.14.6 which is incompatible.\n",
            "pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.14.6 which is incompatible.\n",
            "pymc3 3.11.4 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.14.6 which is incompatible.\n",
            "plotnine 0.6.0 requires numpy>=1.16.0, but you have numpy 1.14.6 which is incompatible.\n",
            "pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.14.6 which is incompatible.\n",
            "numba 0.51.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "mxnet-cu92 1.7.0 requires numpy<2.0.0,>1.16.0, but you have numpy 1.14.6 which is incompatible.\n",
            "librosa 0.8.1 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.14.6 which is incompatible.\n",
            "jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.14.6 which is incompatible.\n",
            "jax 0.3.8 requires numpy>=1.19, but you have numpy 1.14.6 which is incompatible.\n",
            "imgaug 0.2.9 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "fbprophet 0.7.1 requires numpy>=1.15.4, but you have numpy 1.14.6 which is incompatible.\n",
            "fastai 1.0.61 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "cvxpy 1.0.31 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "blis 0.4.1 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.14.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "hA9r0mMC_3Jr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from bert_embedding import BertEmbedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset "
      ],
      "metadata": {
        "id": "SHwqZrZoANyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1oin_Sw1Gk_WLS9zpDrap5FdlRcUrTP_D\n",
        "!gdown 1EvtGQ8-sYXXQ3VA9ByjD4OmFH13WxvAs\n",
        "!gdown 154f-z0PsPAp0yvOLdNXP8NXYjPljpgCZ\n",
        "!gdown 1EDG_j6F5ohIjpkihRQd0-i9daHWV7StN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fpa8eFG7AKkQ",
        "outputId": "aa483623-804d-4d67-b801-cff448013920"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oin_Sw1Gk_WLS9zpDrap5FdlRcUrTP_D\n",
            "To: /content/Sentenses_train.txt\n",
            "100% 1.23M/1.23M [00:00<00:00, 148MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EvtGQ8-sYXXQ3VA9ByjD4OmFH13WxvAs\n",
            "To: /content/Senses_train.txt\n",
            "100% 49.7k/49.7k [00:00<00:00, 39.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=154f-z0PsPAp0yvOLdNXP8NXYjPljpgCZ\n",
            "To: /content/Sentenses_test.txt\n",
            "100% 213k/213k [00:00<00:00, 71.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EDG_j6F5ohIjpkihRQd0-i9daHWV7StN\n",
            "To: /content/Senses_test.txt\n",
            "100% 8.76k/8.76k [00:00<00:00, 8.36MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(part_name):\n",
        "  sentences_file = open(f\"/content/Sentenses_{part_name}.txt\", \"r\")\n",
        "  senses_file = open(f\"/content/Senses_{part_name}.txt\", \"r\")\n",
        "\n",
        "  sentences = sentences_file.readlines()\n",
        "  senses = senses_file.readlines()\n",
        "  \n",
        "  dataset_sentence = []\n",
        "  dataset_sense = []\n",
        "  dataset_ambiguity_word = []\n",
        "\n",
        "  for sentence, sense in list(zip(sentences, senses)):\n",
        "    dataset_sense.append(sense[:-1])\n",
        "    tokens = sentence.split()\n",
        "    word_index = tokens.index(\"<head>\")\n",
        "    dataset_ambiguity_word.append(tokens[word_index+1].lower())\n",
        "    dataset_sentence.append(\" \".join(\n",
        "        tokens[max(0, word_index-11):word_index-1] +\\\n",
        "        [tokens[word_index+1]] +\\\n",
        "        tokens[word_index+3:min(word_index+13, len(tokens))])\n",
        "    )\n",
        "  \n",
        "  return pd.DataFrame({\"Sentence\": dataset_sentence, \"Ambiguity_Word\": dataset_ambiguity_word, \"Sense\": dataset_sense})"
      ],
      "metadata": {
        "id": "lvRliKivDtSj"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = create_dataset(\"train\")\n",
        "test_dataset = create_dataset(\"test\")"
      ],
      "metadata": {
        "id": "wGM-yyXsHMS2"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding"
      ],
      "metadata": {
        "id": "vBPgkfq2DZ0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU\n",
        "# ctx = mx.gpu(0)\n",
        "# bert_embedding = BertEmbedding(ctx=ctx)\n",
        "\n",
        "# CPU\n",
        "bert_embedding = BertEmbedding()"
      ],
      "metadata": {
        "id": "l5Yc9cmJRjMH"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ambiguity_vector(dataset):\n",
        "  dataset[\"Ambiguity_Vector\"] = None\n",
        "  for i, data in dataset.iterrows():\n",
        "    if i%300==0: print(f\"{int(i*100/len(dataset))}%\")\n",
        "    tokens, vectors = bert_embedding([data[\"Sentence\"]])[0]\n",
        "    ambiguity_word = data[\"Ambiguity_Word\"]\n",
        "\n",
        "    try:    \n",
        "      ambiguity_vector = vectors[tokens.index(ambiguity_word)]\n",
        "    except ValueError:\n",
        "      tokens, vectors = bert_embedding([data[\"Sentence\"][15:]])[0]\n",
        "      ambiguity_word = data[\"Ambiguity_Word\"]\n",
        "      ambiguity_vector = vectors[tokens.index(ambiguity_word)]\n",
        "\n",
        "    dataset.at[i, \"Ambiguity_Vector\"] = ambiguity_vector"
      ],
      "metadata": {
        "id": "AgP2y3gHTeJA"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_ambiguity_vector(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noqqFDpATGXL",
        "outputId": "301f0e37-95c3-4789-fc3d-3dafbbc5c28c"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0%\n",
            "4%\n",
            "9%\n",
            "14%\n",
            "18%\n",
            "23%\n",
            "28%\n",
            "33%\n",
            "37%\n",
            "42%\n",
            "47%\n",
            "52%\n",
            "56%\n",
            "61%\n",
            "66%\n",
            "71%\n",
            "75%\n",
            "80%\n",
            "85%\n",
            "90%\n",
            "94%\n",
            "99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_ambiguity_vector(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoT3FYpqJIi4",
        "outputId": "a661e38b-d41a-4cec-aee6-f351d6168077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0%\n",
            "26%\n"
          ]
        }
      ]
    }
  ]
}