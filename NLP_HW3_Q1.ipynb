{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-HW3-Q1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjXcoaByr+owNNTrMGFre3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AUT-Student/NLP-HW3/blob/main/NLP_HW3_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "RoKwrs6nALlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uScG-ckwCRBc",
        "outputId": "49c105c6-7b77-4a54-f737-bca9789e2ae2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert_embedding\n",
            "  Downloading bert_embedding-1.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting numpy==1.14.6\n",
            "  Downloading numpy-1.14.6-cp37-cp37m-manylinux1_x86_64.whl (13.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8 MB 30.1 MB/s \n",
            "\u001b[?25hCollecting gluonnlp==0.6.0\n",
            "  Downloading gluonnlp-0.6.0.tar.gz (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 59.0 MB/s \n",
            "\u001b[?25hCollecting typing==3.6.6\n",
            "  Downloading typing-3.6.6-py3-none-any.whl (25 kB)\n",
            "Collecting mxnet==1.4.0\n",
            "  Downloading mxnet-1.4.0-py2.py3-none-manylinux1_x86_64.whl (29.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.6 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet==1.4.0->bert_embedding) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (3.0.4)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.6.0-py3-none-any.whl size=259930 sha256=cc2aa0e670ffcb8f104a5c71b705b4f1fdcf8d0185628e3c260551d970934781\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/41/8f/45bd1c58055d87aee5a71b6756a427ea8d92e506b3a9d17370\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: numpy, graphviz, typing, mxnet, gluonnlp, bert-embedding\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "yellowbrick 1.4 requires numpy>=1.16.0, but you have numpy 1.14.6 which is incompatible.\n",
            "xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.14.6 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.14.6 which is incompatible.\n",
            "tifffile 2021.11.2 requires numpy>=1.15.1, but you have numpy 1.14.6 which is incompatible.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires numpy>=1.20, but you have numpy 1.14.6 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.14.6 which is incompatible.\n",
            "spacy 2.2.4 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "seaborn 0.11.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.14.6 which is incompatible.\n",
            "pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.14.6 which is incompatible.\n",
            "pymc3 3.11.4 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.14.6 which is incompatible.\n",
            "plotnine 0.6.0 requires numpy>=1.16.0, but you have numpy 1.14.6 which is incompatible.\n",
            "pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.14.6 which is incompatible.\n",
            "numba 0.51.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "librosa 0.8.1 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.14.6 which is incompatible.\n",
            "jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.14.6 which is incompatible.\n",
            "jax 0.3.8 requires numpy>=1.19, but you have numpy 1.14.6 which is incompatible.\n",
            "imgaug 0.2.9 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "fbprophet 0.7.1 requires numpy>=1.15.4, but you have numpy 1.14.6 which is incompatible.\n",
            "fastai 1.0.61 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "cvxpy 1.0.31 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "blis 0.4.1 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed bert-embedding-1.0.1 gluonnlp-0.6.0 graphviz-0.8.4 mxnet-1.4.0 numpy-1.14.6 typing-3.6.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "hA9r0mMC_3Jr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from bert_embedding import BertEmbedding\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset "
      ],
      "metadata": {
        "id": "SHwqZrZoANyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1oin_Sw1Gk_WLS9zpDrap5FdlRcUrTP_D\n",
        "!gdown 1EvtGQ8-sYXXQ3VA9ByjD4OmFH13WxvAs\n",
        "!gdown 154f-z0PsPAp0yvOLdNXP8NXYjPljpgCZ\n",
        "!gdown 1EDG_j6F5ohIjpkihRQd0-i9daHWV7StN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fpa8eFG7AKkQ",
        "outputId": "72bc4fc8-6ec4-4978-b30c-86e470185809"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oin_Sw1Gk_WLS9zpDrap5FdlRcUrTP_D\n",
            "To: /content/Sentenses_train.txt\n",
            "100% 1.23M/1.23M [00:00<00:00, 77.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EvtGQ8-sYXXQ3VA9ByjD4OmFH13WxvAs\n",
            "To: /content/Senses_train.txt\n",
            "100% 49.7k/49.7k [00:00<00:00, 57.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=154f-z0PsPAp0yvOLdNXP8NXYjPljpgCZ\n",
            "To: /content/Sentenses_test.txt\n",
            "100% 213k/213k [00:00<00:00, 86.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EDG_j6F5ohIjpkihRQd0-i9daHWV7StN\n",
            "To: /content/Senses_test.txt\n",
            "100% 8.76k/8.76k [00:00<00:00, 13.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(part_name):\n",
        "  sentences_file = open(f\"/content/Sentenses_{part_name}.txt\", \"r\")\n",
        "  senses_file = open(f\"/content/Senses_{part_name}.txt\", \"r\")\n",
        "\n",
        "  sentences = sentences_file.readlines()\n",
        "  senses = senses_file.readlines()\n",
        "  \n",
        "  dataset_sentence = []\n",
        "  dataset_sense = []\n",
        "  dataset_ambiguity_word = []\n",
        "\n",
        "  for sentence, sense in list(zip(sentences, senses)):\n",
        "    dataset_sense.append(sense[:-1])\n",
        "    tokens = sentence.split()\n",
        "    word_index = tokens.index(\"<head>\")\n",
        "    dataset_ambiguity_word.append(tokens[word_index+1].lower())\n",
        "    dataset_sentence.append(\" \".join(\n",
        "        tokens[max(0, word_index-11):word_index-1] +\\\n",
        "        [tokens[word_index+1]] +\\\n",
        "        tokens[word_index+3:min(word_index+13, len(tokens))])\n",
        "    )\n",
        "  \n",
        "  return pd.DataFrame({\"Sentence\": dataset_sentence, \"Ambiguity_Word\": dataset_ambiguity_word, \"Sense\": dataset_sense})"
      ],
      "metadata": {
        "id": "lvRliKivDtSj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = create_dataset(\"train\")\n",
        "test_dataset = create_dataset(\"test\")"
      ],
      "metadata": {
        "id": "wGM-yyXsHMS2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset[:1200]"
      ],
      "metadata": {
        "id": "f_h6zSocXkhL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding"
      ],
      "metadata": {
        "id": "vBPgkfq2DZ0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU\n",
        "# ctx = mx.gpu(0)\n",
        "# bert_embedding = BertEmbedding(ctx=ctx)\n",
        "\n",
        "# CPU\n",
        "bert_embedding = BertEmbedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Yc9cmJRjMH",
        "outputId": "6cd6eb14-cef2-4cc8-b350-1d41258a5666"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab file is not found. Downloading.\n",
            "Downloading /root/.mxnet/models/book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n",
            "Downloading /root/.mxnet/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ambiguity_vector(dataset):\n",
        "  dataset[\"Ambiguity_Vector\"] = None\n",
        "  for i, data in dataset.iterrows():\n",
        "    if i%300==0: print(f\"{int(i*100/len(dataset))}%\")\n",
        "    tokens, vectors = bert_embedding([data[\"Sentence\"]])[0]\n",
        "    ambiguity_word = data[\"Ambiguity_Word\"]\n",
        "\n",
        "    try:    \n",
        "      ambiguity_vector = vectors[tokens.index(ambiguity_word)]\n",
        "    except ValueError:\n",
        "      tokens, vectors = bert_embedding([data[\"Sentence\"][15:]])[0]\n",
        "      ambiguity_word = data[\"Ambiguity_Word\"]\n",
        "      ambiguity_vector = vectors[tokens.index(ambiguity_word)]\n",
        "\n",
        "    dataset.at[i, \"Ambiguity_Vector\"] = ambiguity_vector"
      ],
      "metadata": {
        "id": "AgP2y3gHTeJA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_ambiguity_vector(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noqqFDpATGXL",
        "outputId": "6647c4ef-92f0-4ffa-e555-f4b19375a3d1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0%\n",
            "25%\n",
            "50%\n",
            "75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_ambiguity_vector(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoT3FYpqJIi4",
        "outputId": "dca3c264-510c-4cb1-d643-d9f24c661cc2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0%\n",
            "26%\n",
            "53%\n",
            "80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimension Reduction"
      ],
      "metadata": {
        "id": "M3I3pghNpV00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=300)"
      ],
      "metadata": {
        "id": "c1fRsHBXpYRY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca.fit(np.array([item for item in train_dataset[\"Ambiguity_Vector\"].values]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfjcsVjkqVX6",
        "outputId": "3f6f4854-b6d0-45a2-e193-21b6c2a17287"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(n_components=300)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca.transform(np.array([item for item in train_dataset[\"Ambiguity_Vector\"].values])).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igV66t4018fO",
        "outputId": "e2653c27-9c3d-43e1-ff3b-9d5d850b1833"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1200, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[\"Ambiguity_Vector_Reduced\"] = [item for item in pca.transform(np.array([item for item in train_dataset[\"Ambiguity_Vector\"].values]))]"
      ],
      "metadata": {
        "id": "jOgKRFlj1aia"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset[\"Ambiguity_Vector_Reduced\"] = [item for item in pca.transform(np.array([item for item in test_dataset[\"Ambiguity_Vector\"].values]))]"
      ],
      "metadata": {
        "id": "Yuu9B5ow2LW5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Classifiers"
      ],
      "metadata": {
        "id": "vE9LeGvuYDgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set(train_dataset[\"Sense\"].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yynx9hOihJ7O",
        "outputId": "aca37f79-36a1-44d7-c155-6f4a7e9fa8c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'HARD1',\n",
              " 'HARD2',\n",
              " 'HARD3',\n",
              " 'SERVE10',\n",
              " 'SERVE12',\n",
              " 'cord',\n",
              " 'division',\n",
              " 'formation',\n",
              " 'interest1',\n",
              " 'interest2',\n",
              " 'interest3',\n",
              " 'interest4',\n",
              " 'interest5',\n",
              " 'interest6',\n",
              " 'phone',\n",
              " 'product',\n",
              " 'text'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set(train_dataset[\"Ambiguity_Word\"].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZQJtynDiimx",
        "outputId": "34c86aed-2d98-488b-a422-288d5bd88529"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hard',\n",
              " 'harder',\n",
              " 'hardest',\n",
              " 'interest',\n",
              " 'interested',\n",
              " 'interesting',\n",
              " 'interests',\n",
              " 'line',\n",
              " 'lines',\n",
              " 'serve',\n",
              " 'served',\n",
              " 'serves'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hard_words = [\"hard\", \"harder\", \"hardest\"]\n",
        "interest_words = [\"interest\", \"interested\", \"interesting\", \"interests\"]\n",
        "line_words = [\"line\", \"lines\"]\n",
        "serve_words = [\"serve\", \"served\", \"serves\"]\n",
        "\n",
        "hard_senses = [\"HARD1\", \"HARD2\", \"HARD3\"]\n",
        "interest_senses = [\"interest1\", \"interest2\", \"interest3\", \"interest4\", \"interest5\", \"interest6\"]\n",
        "line_senses = [\"cord\", \"division\", \"formation\", \"phone\", \"product\", \"text\"]\n",
        "serve_senses = [\"SERVE10\", \"SERVE12\"]\n",
        "\n",
        "mapping_word_sense = {\n",
        "    \"HARD\": (hard_words, hard_senses),\n",
        "    \"INTEREST\": (interest_words, interest_senses),\n",
        "    \"LINE\": (line_words, line_senses),\n",
        "    \"SERVE\": (serve_words, serve_senses)\n",
        "}"
      ],
      "metadata": {
        "id": "MbFhDMIZhr9j"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def report_accuracy_f(classifier, dataset):\n",
        "  y_predict = classifier.predict(np.stack(dataset[\"Ambiguity_Vector_Reduced\"].values))\n",
        "  y_true = dataset[\"Sense\"].values\n",
        "\n",
        "  print(f\"Accuracy = {accuracy_score(y_true, y_predict)}\")\n",
        "  print(f\"F1 = {f1_score(y_true, y_predict, average='macro')}\")  "
      ],
      "metadata": {
        "id": "YiKyO1IDBOrU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = {}\n",
        "\n",
        "for key in mapping_word_sense.keys():\n",
        "  print(key)\n",
        "  words = mapping_word_sense[key][0]\n",
        "  senses = mapping_word_sense[key][1]\n",
        "\n",
        "  sub_dataset = train_dataset[train_dataset[\"Ambiguity_Word\"].isin(words) & train_dataset[\"Sense\"].isin(senses)]\n",
        "\n",
        "  classifier = SVC()\n",
        "\n",
        "  if key != \"HARD\":\n",
        "    classifier.fit(X=np.stack(sub_dataset[\"Ambiguity_Vector_Reduced\"].values), y=sub_dataset[\"Sense\"].values)\n",
        "    report_accuracy_f(classifier, sub_dataset)\n",
        "    classifiers[key] = classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7Fd83Yu4huD",
        "outputId": "0e4eee18-5a66-4c87-836c-4437f4c3df9e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HARD\n",
            "INTEREST\n",
            "Accuracy = 0.9563758389261745\n",
            "F1 = 0.7550486797767185\n",
            "LINE\n",
            "Accuracy = 0.9711191335740073\n",
            "F1 = 0.96479310543866\n",
            "SERVE\n",
            "Accuracy = 1.0\n",
            "F1 = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Uhb3MMyTEjQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xtrRonhAwKT",
        "outputId": "13f63f2e-8609-490f-ff86-00ef929632fe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'INTEREST': SVC(), 'LINE': SVC(), 'SERVE': SVC()}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_dataset[\"Sense\"].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmZIcdGyg3lK",
        "outputId": "cde4eddc-888b-4039-9f3b-d14f2819ecd0"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['interest6', 'interest3', 'interest6', 'interest6', 'interest6',\n",
              "       'interest3', 'interest6', 'interest6', 'interest6', 'interest5',\n",
              "       'interest5', 'interest6', 'interest6', 'interest1', 'interest6',\n",
              "       'interest1', 'interest6', 'interest6', 'interest6', 'interest6',\n",
              "       'interest4', 'interest6', 'interest6', 'interest6', 'interest6',\n",
              "       'interest6', 'interest1', 'interest5', 'interest6', 'interest6',\n",
              "       'interest6', 'interest1', 'interest6', 'interest1', 'interest6',\n",
              "       'interest5', 'interest6', 'interest6', 'interest6', 'interest6',\n",
              "       'interest6', 'interest6', 'interest5', 'interest6', 'interest1',\n",
              "       'interest6', 'interest1', 'interest6', 'interest5', 'interest6',\n",
              "       'interest6', 'interest6', 'interest6', 'interest1', 'interest5',\n",
              "       'interest6', 'interest5', 'interest6', 'interest1', 'interest5',\n",
              "       'interest1', 'interest1', 'interest6', 'interest6', 'interest1',\n",
              "       'interest5', 'interest1', 'interest1', 'interest6', 'interest5',\n",
              "       'interest6', 'interest6', 'interest6', 'interest6', 'interest4',\n",
              "       'interest5', 'interest6', 'interest6', 'interest6', 'interest6',\n",
              "       'interest6', 'interest4', 'interest6', 'interest6', 'interest5',\n",
              "       'interest1', 'interest5', 'interest6', 'interest6', 'interest6',\n",
              "       'interest6', 'interest6', 'interest6', 'interest1', 'interest6',\n",
              "       'interest6', 'interest4', 'interest5', 'interest5', 'interest6',\n",
              "       'interest6', 'interest1', 'interest5', 'interest6', 'interest3',\n",
              "       'interest6', 'interest6', 'interest5', 'interest4', 'interest5',\n",
              "       'interest6', 'interest1', 'interest6', 'interest1', 'interest6',\n",
              "       'interest6', 'interest5', 'interest1', 'interest6', 'interest6',\n",
              "       'interest6', 'interest1', 'interest6', 'interest6', 'interest5',\n",
              "       'interest6', 'interest6', 'interest6', 'interest5', 'interest4',\n",
              "       'interest5', 'interest6', 'interest6', 'interest4', 'interest6',\n",
              "       'interest6', 'interest6', 'interest6', 'interest2', 'interest3'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svc.predict(np.stack(sub_dataset[\"Ambiguity_Vector_Reduced\"].values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtTVCaeCfuU2",
        "outputId": "1b398460-1c17-4e42-e77d-044b8a3b21e7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['interest6', 'interest1', 'interest6', 'interest6', 'interest6',\n",
              "       'interest3', 'interest6', 'interest6', 'interest6', 'interest5',\n",
              "       'interest5', 'interest6', 'interest6', 'interest1', 'interest6',\n",
              "       'interest1', 'interest6', 'interest6', 'interest6', 'interest6',\n",
              "       'interest4', 'interest6', 'interest6', 'interest6', 'interest6',\n",
              "       'interest6', 'interest5', 'interest6', 'interest6', 'interest6',\n",
              "       'interest6', 'interest1', 'interest6', 'interest1', 'interest6',\n",
              "       'interest6', 'interest6', 'interest6', 'interest6', 'interest6',\n",
              "       'interest6', 'interest6', 'interest5', 'interest6', 'interest1',\n",
              "       'interest6', 'interest1', 'interest6', 'interest5', 'interest6',\n",
              "       'interest6', 'interest6', 'interest6', 'interest1', 'interest5',\n",
              "       'interest6', 'interest6', 'interest6', 'interest1', 'interest5',\n",
              "       'interest1', 'interest1', 'interest6', 'interest6', 'interest1',\n",
              "       'interest5', 'interest1', 'interest1', 'interest6', 'interest5',\n",
              "       'interest6', 'interest6', 'interest6', 'interest6', 'interest4',\n",
              "       'interest5', 'interest6', 'interest6', 'interest6', 'interest6',\n",
              "       'interest6', 'interest4', 'interest6', 'interest6', 'interest5',\n",
              "       'interest1', 'interest5', 'interest6', 'interest6', 'interest6',\n",
              "       'interest6', 'interest6', 'interest6', 'interest1', 'interest6',\n",
              "       'interest6', 'interest4', 'interest5', 'interest5', 'interest6',\n",
              "       'interest6', 'interest1', 'interest5', 'interest6', 'interest1',\n",
              "       'interest6', 'interest6', 'interest5', 'interest5', 'interest5',\n",
              "       'interest6', 'interest1', 'interest6', 'interest1', 'interest6',\n",
              "       'interest6', 'interest5', 'interest1', 'interest6', 'interest6',\n",
              "       'interest6', 'interest1', 'interest6', 'interest6', 'interest5',\n",
              "       'interest6', 'interest6', 'interest6', 'interest5', 'interest4',\n",
              "       'interest5', 'interest6', 'interest6', 'interest4', 'interest6',\n",
              "       'interest6', 'interest6', 'interest6', 'interest1', 'interest3'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B-RPJ1RCgxtF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}